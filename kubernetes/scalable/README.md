Overview
========

This set of configurations differs from that of the 'simple' directory in that it splits anchore-engine into two different kubernetes deployment definitions.

The 'core' deployment definition is a single pod comprising the main API services:
* external api
* catalog
* queues
* policy_engine
* kubernetes webhook handler

These services tend to be low resource consumption are not a primary block on analysis throughput. This deployment has a corresponding service definition to enable the other deployment to access the various services via the cluster IP.

It is possible to further split this deployment in two again to deploy the policy-engine service on its own, 
but that is an exercise left for later but will follow the same pattern seen here (basically a service definition, deployment definition, and a config.yaml with only policy-engine enabled).

The second service, the 'analyzer' service can be scaled-out and includes a single pod definition for the analyzer processes that
actually do the image fetch, unpack, and analysis. These workers are the primary resource consumers in the process and allowing parallelism increases system throughput.


Persistence
===========

A single postgresql db is required and expected in these example configs to be deployed as 'anchore-db-postgresql' service and thus available in environment variables and secrets using that prefix (we installed using a helm chart, for example). The provided deployment definitions assume that the postgres db is deployed such that pods have $ANCHORE_DB_POSTGRESQL_SERVICE_HOST and $ANCHORE_DB_POSTGRESQL_SERVICE_PORT available in the pod environment as provided by kubernetes. This requires that the db service is deployed before the anchore services so that the service definition is up and configured.
The other specific aspect of this config example is that the postgres db user is 'postgres' and the postgres db password is provided in a secret `anchore-db-postgresql` with key `postgres-password`. This is the default way that the helm chart stable/postgresql provides the credentials, but can easily be modified using a different set of secrets by configuring the anchore-engine-core-deployment.yaml and anchore-engine-analyzer-deployment.yaml files to set the ANCHORE_DB_USER and ANCHORE_DB_PASSWORD environment variables. This example also provides an anchore-db-secrets.yaml that could be used in place of a secret generated by the db install itself.

Deploy postgresql db as a service via helm or your preferred db deployment mechanism:
```
helm install anchore-db stable/postgresql
```

Anchore Engine Core
===================

The Anchore Engine Core service provides the external api, queueing system, and policy engine as well as the optional kubernetes webhook handler service (for imagepolicywebhook handling) that external clients will interact with. This is service does not process images for analysis directly so is fairly lightweight. It is configured as a single pod and should always have only a single replica (there are stateful coordinators in some services contained in this pod).

Create the anchore-engine-core service:
```
cp core-config.yaml config.yaml
<set admin password in anchore-engine-users-secrets.yaml using b64 encoded password you want to use>
kubectl create configmap anchore-core-config --from-file=config.yaml
kubectl craete -f anchore-engine-users-secrets.yaml
kubectl create -f anchore-engine-core-service.yaml
rm config.yaml
```

Create the anchore-engine-core deployment: 
```
kubectl create -f anchore-engine-core-deployment.yaml
```

Anchore Engine Analyzers
========================

The Anchore Engine Analyzer deployment is the set of worker pods that perform image download, extraction, and analysis and do most of the heavy lifting for the system. These can be scaled out horizontally to meet throughput requirements and are feed work from the job queues managed by the core service. Because these pods require access to the docker socket there isn't much value in running more than one per host as it can easily overwhelm the docker daemon on the host and cause bottlenecks. We recommend using kubernets pod affinity and anti-affinity scheduling rules to spread these pods across your cluster and you may also want to dedicate some hosts to their use by using selector labels to isolate them given the io load they impose on the host.

Also, depending on what host OS you're running k8s on, you may want to mount an external volume to provide data for these pods. The default Centos install of docker uses the device-mapper storage driver which has a default size limit of 10GB for a container. The rule of thumb for a worker is 3x the size (uncompressed) of the largest image you want to analyze is required to allow extraction and analysis. If you mount a volume to /root/.anchore the system will use that for temporary space. This storage is ephemeral in usage so it does not need to be persisted after pod termination. It is purely scratch space used for image analysis.

Create the anchore-engine-analyzer deployment: 
```
cp worker-config.yaml config.yaml
kubectl create configmap anchore-worker-config --from-file=config.yaml
kubectl create -f anchore-engine-analyzer-deployment.yaml
rm config.yaml
```

As needed you can scale the deployment (e.g. kubectl scale --replicas=3 deployment/anchore-engine-analyzer-deployment)

